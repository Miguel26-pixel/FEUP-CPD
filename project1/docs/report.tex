\documentclass{report}

\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{adjustbox}
\graphicspath{{graphics/}}


\begin{document}
	\begin{titlepage}
		\centering
		\includegraphics[scale=0.35]{logo_feup.png}\linebreak
		
		\vspace{1cm}
		
		{\scshape \large Bachelor in Informatics and Computing Engineering
		Information}
		
		\vspace {1cm}
		
		{\scshape\Huge Performance evaluation of a single core \par}
		
		\vfill
		
		{\scshape \large Parallel and Distributed Computing}
		
		\vfill
		
		\Large David \textsc{Preda} - up201904726 \\ Fernando \textsc{Rego} - up201905951 \\ Miguel \textsc{Amorim} - up201907756
		
		\vspace{1cm}
		
		\today
		
	\end{titlepage}

	\tableofcontents
	
	\chapter{Problem Description}
			\paragraph{} The memory system is a hierarchy of storage devices with different capacities, costs, and access times. This hierarchy can be abstracted as a triangle, where the bottom represents the cheaper storage devices with a larger but slower amount of memory. On the other hand, the higher levels of the triangle represent storage devices with small capacities but with a much faster access time.
			
			\paragraph{}Memory hierarchies perform as well as they due to caching. The faster levels of memory keep recently used values in order to enable faster access to them, which drastically decreases the time spent fetching data from memory. On top of this, this organization pattern allows to diminish costs, as the larger part of memory can keep being slower with much less impact on performance.
			
			\paragraph{} This project aims to study the effect of the memory hierarchy on the processor performance when accessing large amounts of data. Three different algorithms for the product of two matrices will be approached, given that this problem usually deals with many memory accesses. 
			
			\paragraph{}Finally, during the execution of each of the algorithms, several metrics will be collected, so conclusions can be made at the end of this report.
			
			
	\chapter{Algorithms Explanation}
	
		\paragraph{}The following algorithms all solve the problem of multiplying two matrices. However, each algorithm presented is intended to have memory access have a lesser effect on performance than the previous ones. 
	
		\section{Dot Product Algorithm}
		
			\paragraph{}The simplest algorith comes directly from the math definition of matrix multiplication, $C = A \cdot B$. Each line of the first matrix is multiplied by each column of the second matrix. Extending the math definition, we can obtain:
			
			\begin{center}
				$C_{ij} = \sum_{k=1}^{m} A_{ik} \cdot B_{kj}$
			\end{center}
		
			\paragraph{} From the aforementioned, a simple algorithm can be developed using nested loops over the indices \emph{i}, \emph{j} and \emph{k} to attain the product matrix:
			
			\begin{algorithm}[H]
				\caption{Matrix Multiplication} 
				\begin{algorithmic}[1]
					\State$A\gets $ First Matrix; $B\gets $ Second Matrix;
					\For {$i=0,1,\ldots,size(A)$}
						\For {$j=0,1,\ldots,size(B)$}
							\State $temp\gets $ 0;
							\For {$k=0,1,\ldots,size(A)$}
								\State $temp += A_{ik} \cdot B_{kj}$
							\EndFor
							\State $C_{ij} += temp$
						\EndFor
					\EndFor
					\State\Return $C$;
				\end{algorithmic} 
			\end{algorithm}
		
		\section{Line Matrix Multiplication}
		
			\paragraph{} The line matrix multiplication algorithm is very similar to the first algorithm. The only difference is that, instead of multiply one line of the first matrix by each column of the second matrix, this version multiplies one single element from the first matrix by the correspondent line of the second matrix. 
			
			\paragraph{} To program this algorithm, it is possible to depart from the previous one and change the order of the nested for loops. The following pseudocode makes this more explicit:
			
			\begin{algorithm}[H]
				\caption{Line Matrix Multiplication} 
				\begin{algorithmic}[1]
					\State$A\gets $ First Matrix; $B\gets $ Second Matrix;
					\State$C\gets $ Matrix initialized with 0's;
					\For {$i=0,1,\ldots,size(A)$}
						\For {$k=0,1,\ldots,size(A)$}
							\For {$j=0,1,\ldots,size(B)$}
								\State $C_{ij} += A_{ik} \cdot B_{kj}$
							\EndFor
						\EndFor
					\EndFor
					\State\Return $C$;
				\end{algorithmic} 
			\end{algorithm}
		
		\section{Block Matrix Multiplication}
		
		\paragraph{} The block matrix multiplication algorithm consists in partitioning the initial matrix into blocks or sub-matrices of a given size. Finally with the partitioned matrix, we can perform the multiplication of the two initial matrix by multiplying the sub-matrices of smaller size.
		
		\paragraph{} After dividing the matrices in blocks there is two possible approaches. The first approach consist in the dot product, used in the matrix multiplication algorithm, and the second in the line product, used in the line matrix multiplication algorithm. Our team opted to use the line product since this approach have substantial gains when compared to the first. The algorithm can be represented with the following pseudocode:
		
		\begin{algorithm}[H]
			\caption{Line Matrix Multiplication} 
			\begin{algorithmic}[1]
				\State$A\gets $ First Matrix; $B\gets $ Second Matrix;
				\State$C\gets $ Matrix initialized with 0's;
				\State$N\gets size(A)$;
				\State$b\gets $ Block size;
				\For {$ii=0,b,\ldots,N$}
					\For {$jj=0,b,\ldots,N$}
						\For {$kk=0,b,\ldots,N$}
							\For {$i=ii,ii+1,\ldots,ii+b$}
								\For {$j=jj,jj+1,\ldots,jj+b$}
									\For {$k=kk,kk+1,\ldots,kk+b$}
										\State $C_{ik} += A_{ij} \cdot B_{jk}$
									\EndFor
								\EndFor
							\EndFor
						\EndFor
					\EndFor
				\EndFor
				\State\Return $C$;
			\end{algorithmic} 
		\end{algorithm}
	
	\chapter{Results and Analysis}
	
		\paragraph{}As requested for this project, the dot product and the line multiplication algorithms were implemented in two languages - C++ and Rust. The block multiplication algorithm was only implemented in C++.
		
		\paragraph{}It is worth noting that the Rust version is using a slightly less efficient implementation than in the C++ one. However, given that the aim of the project is to analyse the effects of memory access on the performance of a single core and not the direct comparison of both languages, the authors believe that this is not a major issue.
	
		\section{Performance Metrics}
		
			\paragraph{}In order to correctly evaluate performance and the correlation between memory access and execution time,  the \emph{Performance Application Programming Interface} (or \emph{PAPI}, for short) was used to collect the number of data cache misses on the L1 and L2 cache level.
			
			\paragraph{}The authors original intention was to measure the percentage of data cache misses relative to the total of data cache accesses. However, due to hardware limitations, \emph{PAPI} wasn't able to register these values. Although this metric could have been of more value to the study, but, sadly, it was not possible to present them.
			
			\paragraph{}Regardless, the execution time for each of the algorithms was also registered, without the use of any external API.
			
			\clearpage
			
		\section{Testing Hardware}
		
			\paragraph{CPU}- Intel i5-9300H (8) @ 4.100GHz
			\paragraph{RAM}- 20Gb DDR4 Synchronous 2667 MHz
			\paragraph{CACHE L1}- 256KiB
			\paragraph{CACHE L2}- 1MiB 
			
		\section{Algorithm Comparison}
		
			\paragraph{}As a way of avoiding any possible misunderstanding, the reader should consider that whenever \emph{cache} is mentioned, the authors mean, specifically, the data cache. Whenever another type of cache, such as the instructions cache, is being mentioned, it should be explicitly clarified.
		
			\subsection{Dot Product vs Line Multiplication}
			
				\paragraph{}Let us start our analysis by comparing the cache misses at both the L1 and L2 levels.
			
				\begin{figure}[H]
					\begin{adjustbox}{center}
						\includegraphics[scale=0.4]{cpp_dot_line_l1.png}
						\includegraphics[scale=0.4]{rs_l1_misses.png}
					\end{adjustbox}
					\caption{Comparison of the number of L1 data cache misses of the dot product and line multiplication algorithms for matrices of increasing size. To the left: C++ implementation; to the right: Rust implementation.}
				\end{figure}
			
				\begin{figure}[H]
					\begin{adjustbox}{center}
						\includegraphics[scale=0.4]{cpp_dot_line_l2.png}
						\includegraphics[scale=0.4]{rs_l2_misses.png}
					\end{adjustbox}
					\caption{Comparison of the number of L2 data cache misses of the dot product and line multiplication algorithms for matrices of increasing size. To the left: C++ implementation; to the right: Rust implementation.}
				\end{figure}
			
				\paragraph{}As it is pretty straightforward to see, at both level of caches there is a significant reduction on the number of cache misses from the dot product to the line multiplication algorithm.
				
				\paragraph{}In theory, this should mean a reduced execution time for the line multiplication algorithm, given that it does not have to go through the overhead of higher-level memory access as often as the dot product algorithm.
				
				\paragraph{}Let us now compare the execution time of both algorithms:
			
				\begin{figure}[H]
					\begin{adjustbox}{center}
						\includegraphics[scale=0.4]{cpp_dot_line_comparison.png}
						\includegraphics[scale=0.4]{rs_algorithm_comparison.png}
					\end{adjustbox}
					\caption{Comparison of execution time of the dot product and line multiplication algorithms for matrices of increasing size. To the left: C++ implementation; to the right: Rust implementation.}
				\end{figure}
			
				\paragraph{}As expected, the line multiplication algorithm is able to perform all of its operations in a much smaller time, going from 134.34 seconds to only 18.27 seconds for 3000x3000 matrices.
			
			\subsection{Block Multiplication with Different Sizes}
			
				\paragraph{}For the block multiplication algorithm, let us now analyse the metrics inversely - starting with the execution time:
				
				\begin{figure}[H]
					\begin{adjustbox}{center}
						\includegraphics[scale=0.5]{cpp_block_comparison.png}
					\end{adjustbox}
					\caption{Execution time of the block multiplication algorithm for increasing block and matrix sizes.}
				\end{figure}
			
			
				\paragraph{}From the figure, one can conclude that, on average, the 512 block size version of the algorithm performs better than the others.
				
				\paragraph{}With this in mind, let us now analyse the cache misses per millisecond:
				
				\begin{figure}[H]
					\begin{adjustbox}{center}
						\includegraphics[scale=0.4]{cpp_block_l1_misses.png}
						\includegraphics[scale=0.4]{cpp_block_l2_misses.png}
					\end{adjustbox}
					\caption{Comparison of data cache misses of the block multiplication for increasing block and matrix sizes. To the left: L1 cache misses per millisecond; to the right: L2 cache misses per millisecond.}
				\end{figure}
			
				\paragraph{}Contrary to the expected, the 512 block size version does not have a lower cache miss per millisecond rate. Therefore, it is not possible to relate the better performance with the accesses to memory.
				
				\paragraph{}However, after some research, the authors concluded that the better performance could be related to computational intensity. Bundling together arithmetic operations can lead to faster performance, depending on the cache size of the underlying hardware and the load of the operations. \cite{1}
		
			\subsection{Line Multiplication vs Block Multiplication}
				
				\paragraph{}Finally, let us compare the line multiplication algorithm with block multiplication algorithm. As it performed better, the 512 block size version will be the one used to represent the block approach.
				
				\paragraph{}As in the first comparison, let's first verify how both algorithms behave cache-wise:
				
				\begin{figure}[H]
					\begin{adjustbox}{center}
						\includegraphics[scale=0.4]{cpp_line_block_l1.png}
						\includegraphics[scale=0.4]{cpp_line_block_l2.png}
					\end{adjustbox}
					\caption{Comparison of data cache misses between the line multiplication algorithm and the block multiplication algorithm for increasing matrices. To the left: Total L1 data cache misses; to the right: Total L2 data cache misses.}
				\end{figure}
			
				\paragraph{}The total L1 data cache misses are drastically smaller for the block multiplication algorithm. However, they are almost on par at the L2 cache level, with the line multiplication having less misses.
				
				\paragraph{}Let us now see how if there is any correlation with the execution time:
				
				\begin{figure}[H]
					\begin{adjustbox}{center}
						\includegraphics[scale=0.5]{cpp_line_block_comparison.png}
					\end{adjustbox}
					\caption{Comparison of execution time between the line multiplication algorithm and block multiplication algorithm for increasing matrices.}
				\end{figure}
			
				\paragraph{}The block multiplication algorithm, as the matrices increase in size, starts to take less and less time, when comparing it with the line multiplication algorithm. This difference is analogous to what is seen on the graph comparing the L1 data cache misses between the two algorithms.
				
				\paragraph{}We, therefore, conclude that the block approach leads to a better performance due to being less heavy on higher-level memory access.
			
		\section{Language Comparison}
		
			\paragraph{}Finally, a comparison between both languages is now due. The line multiplication algorithm will be used as the basis for this comparison, as it is the best performing one that is implemented in both language. 
			
			\paragraph{}Let us start by comparing data cache misses: 
			
			\begin{figure}[H]
				\begin{adjustbox}{center}
					\includegraphics[scale=0.4]{line_l1_cache_misses.png}
					\includegraphics[scale=0.4]{line_l2_cache_misses.png}
				\end{adjustbox}
				\caption{Comparison of data cache misses of the line multiplication algorithm between the Rust and C++ implementations for increasing matrices. To the left: Total L1 data cache misses; to the right: Total L2 data cache misses.}
			\end{figure}
		
			\paragraph{}For the L1 level, the languages are essentially tied. However, Rust performs slightly worse at the L2 level.
			
			\paragraph{}The graph below shows that this slight difference translates into a better performance for the C++ implementation. However, it should also be taken into consideration that the implementations are not entirely similar, with structures with more overhead being used on the Rust implementation.
			
			\begin{figure}[H]
				\begin{adjustbox}{center}
					\includegraphics[scale=0.5]{line_algorithm_comparison.png}
				\end{adjustbox}
				\caption{Comparison of the execution time of the line multiplication algorithm between the Rust and C++ implementations for increasing matrices.}
			\end{figure}
		
			\paragraph{}It can be concluded that C++ performed better, even though this wasn't the fairest of comparisons.
	
	\chapter{Conclusions}
	        
	        \paragraph{}This project allowed us to deeply understand some computer concepts as memory hierarchy and its performance with a lot of memory accesses.
	        
	        \paragraph{}The creation and application of the three different algorithms for the product of two matrices made us acknowledge the effect of the memory hierarchy on the processor performance, when accessing large amount of data, since each algorithm has different levels of how faster levels of memory are accessed.
	
	\addcontentsline{toc}{chapter}{Bibliography}
	\begin{thebibliography}{1}
		\bibitem{1}
		JAYAWEERA, Malith \textit{Blocked Matrix Multiplication}
		\\\texttt{https://malithjayaweera.com/2020/07/blocked-matrix-multiplication/}
	\end{thebibliography}
	
\end{document}