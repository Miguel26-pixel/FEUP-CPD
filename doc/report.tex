\documentclass{report}

\usepackage[obeyspaces]{url}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{adjustbox}
\graphicspath{{graphics/}}

\begin{document}
	\begin{titlepage}
		\centering
		\includegraphics[scale=0.35]{logo_feup.png}\linebreak
		
		\vspace{1cm}
		
		{\scshape \large Bachelor in Informatics and Computing Engineering}
		
		\vspace {1cm}
		
		{\scshape\Huge Distributed and Partitioned Key-Value Store \par}
		
		\vfill
		
		{\scshape \large Parallel and Distributed Computing}
		
		\vfill
		
		\Large David \textsc{Preda} - up201904726 \\ Fernando
		\textsc{Rego} - up201905951 \\ Miguel \textsc{Amorim} - up201907756
		
		\vspace{1cm}
		
		\today
		
	\end{titlepage}

	\tableofcontents
	
	\chapter{Problem Description}
			\paragraph{} A key-value store is a simple storage system that stores
			arbitrary data objects, the values, each of which is accessed by means
			of a key, very much like in a hash table. To ensure persistency, the data
			items and their keys must be stored in persistent storage, e.g. a hard
			disk drive (HDD) or a solid state disk (SSD), rather than in RAM.
			
			\paragraph{}By distributed, we mean that the data items in the key-value
			store are partitioned among different cluster nodes.
			
			\paragraph{} Our design is loosely based on Amazon's Dynamo, in that it
			uses consistent-hashing to partition the key-value pairs among the 
			different nodes.
	
	\chapter{Message Format}

			\paragraph{}
	        
	        \paragraph{}

	\chapter{Membership Service}
			\emph{The message format is detailed described in the Chapter 2}

	        \paragraph{}
	        
	        \paragraph{}

			\section{Implementation}
	
				\paragraph{}
				
				\paragraph{}

			\section{RMI}
				\paragraph{} The RMI (Remote Method Invocation) was used for both
				membership operations, join and leave. The file with the definition 
				of the remote interface can be found at the following path:

				\begin{center}
					\path{g05/assign2/src/client/Services.java}
				\end{center}
				
	\chapter{Key-value Store}
			\emph{The message format is detailed described in the Chapter 2}
	
	        \paragraph{}
	        
	        \paragraph{}

			\section{Implementation}
	
				\paragraph{}
				
				\paragraph{}

				\subsection{Put}
				\subsection{Get}
				\subsection{Delete}
	
	\chapter{Replication}
	
	        \paragraph{} Replication is an important factor for distributed computing
			as it increases the availability of a service, in this case a file,
			throughout the distributed network.
	        
			\section{Implementation}
	
				\paragraph{} Implementing replication consists of keeping a factor 
				of 3, by another words, each key-value pair must be on 3 nodes of 
				the cluster. This allow the client to always obtain the key-value 
				pair, even if one of the nodes is down, because there will be another
				node with the pretended key-value. 

				\paragraph{} Therefore, for each operation, the necessary tools for 
				keeping the replication factor were implemented:
				
				\subsection{Put}
					\paragraph{} Upon a put operation, the value associated to the
					operation is saved in the first active node according to the 
					consistent hashing, and then, if possible, the file is replicated to 
					the next two active nodes in the cluster.

				\subsection{Get}
					\paragraph{} Upon a get operation, the responsible node is calculated
					according to the consistent hashing, and then request of the value
					is sent to obtain the pretended value. If, for some reason, the 
					value can't be obtained from that node, the same request 
					is made to one of the next two nodes that follows the responsible 
					node.

				\subsection{Delete}
					\paragraph{} Upon a delete operation, the responsible node is calculated
					according to the consistent hashing, and then request to delete a
					key-value pair is sent to responsible node as well to the two next 
					active nodes in the cluster.

				\subsection{Join and Get}
					\paragraph{} Upon a join or a get membership operation, for each key-value 
					pair, the nodes that will store the pair or the replicated pairs are
					recalculated to make the necessary changes of key-value pairs between
					the nodes to mantain the replication factor of 3 key-value pairs.

			\section{Implications on membership and storage devices}
	
				\paragraph{} The implementation of the replication involved both membership
				and storage devices and forced some significant changes to this devices.
				
				\paragraph{} To reach the full potential of the replication, the membership
				should always be updated to minimize any error when the nodes are calculated
				to perform any operation of the storage. This had some implications of
				operation synchronization in order to keep the replication factor of
				3 and to store the key-value pairs in the right nodes and minimize the 
				probability of errors occurring to the maximum.

	\chapter{Fault-Tolerance}
	
	        \paragraph{}
	        
	        \paragraph{}

	\chapter{Thread-pools}
	
	        \paragraph{}
	        
	        \paragraph{}

			\section{Implementation}
	
				\paragraph{}
				
				\paragraph{}

	\chapter{Conclusions}
	
	        \paragraph{}This project allowed us to deeply understand some computer concepts
			related to distributed systems and force us to solve several different type of 
			problems. 
	        
	        \paragraph{} The development of this project provided us important knowledge
			about communication protocols, such as TCP and UDP, as well the implementation and
			the usage of these protocols. Allied to the communication, the synchronization 
			problems that appear during the development of this project, force us to think 
			\emph{out of the box} to solve an variaty of different problems. Finally, this 
			project gave us a little idea of how large distributed systems are implemented
			in real world scenarios. 

\end{document}
